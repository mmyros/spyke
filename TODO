RECENT:

    - stop adding of new ellipsoid (which are now glyphs instead of parametric surfaces) from resetting the camera view

    - colour items in nlist and clist - declare OnGetItemAttr method (see wx demos)
        - do it in the cluster list as well - maybe make that a proper listctrl
        - this would require a black background for the listctrl, otherwise white wouldn't show up

    - always assuming the first peak is -ve, and the 2nd is +ve - is this really a safe assumption all the time? Seemed to be for ptc15, but for the other cats?

    - nid tooltips are a bit slow and not always reliable

    - maybe keep rendering disabled during the whole .sort restore process - it's currently disabled when calling mlab.points3d

    - should really set up neuropy to import the spikes from the .sort directly, but can make it still rely on an existing .din and .textheader file, since those would have to be pulled from the .srf file anyway

    - rethink the whole "best" thing for marking which sort to use - maybe just leave it be with no best marker any more, and add a sort level to the command line neuropy interface - nevermind, its already there, but the sorts are only numbered, not named
        - also, you can't go sortN.e0.sta(), since sort is below exp in the hierarchy - this is awkward - rethink the hierarchy?
        - tree printout should show the sorts and experiments?
    - get neuropy+dimstim working in Python 2.6

    - make neuron renumbering (to get rid of gaps in IDs) work again
        - add a feature where you renumber the neurons according to their vertical spatial order - this would make STA plots come out vertically ordered, which would be nice for deducing layers and where you get lots of simple fields and complex fields

    - load a huge .sort with a huge .wave, then loading it again seems to run out of memory, even though I'm doing my best to delete the wavedata and garbage collect before loading from a .wave - must be some stray references to the wavedata hanging around somewhere

    - MAYBE: add ability to turn on/off a cluster (and its coloured points) temporarily, so you can see the uncoloured density of points within it, and maybe better decide how to adjust the cluster to match

    - overplotting speed

    - actually, not so bad: need to figure out what to do with not-so-great grey and white cluster colours - hard to see which points are inside/outside such coloured ellipses (especially in the white ellipses) - maybe make their points greyish red and pink (respectively) instead?

    - manual sorting is disabled, and maybe it should stay that way

    - restoring from .sort with no .srf open is broken again

    - is there some way to speed up "from enthought.mayavi import mlab"? - painful!

    - looks like figuring out which points fall within a cluster still doesn't work right when you're orienting all 3 dims at the same time - or at the very least, there's some kind of round-off error going on

    - might be good to finally implement proper simultaneous spatiotemporal detection
        - search through 2D array representing window of data, look for all extrema
        - sort extrema by amplitude
        - for extremum in extrema: # biggest to smallest
            - check lockout
            - if this next biggest extremum is locked out, break out of loop?
            - find corresponding phase of this extremum on the same chan, update spatiotemporal lockout
        - would be more reliable for multiple spikes in close proximity, by always giving the biggest one precedence
        - might also be faster than current approach of search across time, then across chans, then across time...

    - parameter extraction should be relatively easy to parallelize - it's a purely CPU (or memory?) limited operation

    - deleting a detection doesn't work right now, and might now be a bit of a hassle - if len(sort.wavedatas) > 1, concatenate them all into a single array, slice out the keepris, and save that as the new single wavedatas[0]. If it fails due to MemoryError, too bad. Deleting a detection is rare anyway, especially a big detection

    - ptc15.81 not contiguous at record timestamps number 3309 and 3310
        np.where((np.diff(s.stream.rts, n=2) == 0) == False)
        (array([3309, 3310]),)
        - when stream is asked for gap in data, fill the gap with zeros
        - fill the same gap with zeros in .resample file
            - find ctsrecord duration that's the most frequent

    - replace unnecesary use of list(set(array)) with np.unique(array)

    - add some Sort method that restores the .wavedatas list given the current set of .spikes and an open .srf, if no wavedata were present in a .sort file that was restored from
        - something like Sort.restore_wavedata()

    - open .sort with no .srf, close it, open a .srf, get no sort.st attrib error when updating rasters - no slider shows up as a result?
        - maybe soln is to turn off rasters every time you open a .srf, and then enable them every time you open a .sort?

    - detect, cluster, do new detection just following first one in time, cluster, then delete first one, get an IndexError

    - deleting a detection doesn't clear neurons' waveforms, but maybe that's not such a bad thing?

    - selecting a block of spikes in list doesn't plot them - might be due to bugfixes in latest wx, might not need my hack workarounds anymore?

    - when deleting a detection, if ndetections == 0, reset _detid = 0. Maybe do same for nids and sids as well?

    - clear/update neuron mean waveform and replot it immediately (if currently plotted), when updating its cluster members, ie moving/modifying the cluster and then hitting "c"
        - close .srf file, open test.sort with no waves in it, modify cluster, apply it, yet mean neuron waveform remains unchanged from when it was last calculated when the .srf file was open

    - fix LFP

    - maybe before saving a .sort, check if it has detections/spikes from another .srf, if so, ask user if that's the intention




GENERAL:

- reduce number of chans included in a spike according to amount of signal on that signal, maybe based on median signal for that chan for entire file, or randomly sampled subset of the file

- order colours consecutively according to cluster mean y location, to make neighbouring clusters in X-Y space less likely to be assigned the same colour

- WARNING! TODO: not sure if say ADchan 4 will always have a delay of 4us, or only if it's preceded by AD chans 0, 1, 2 and 3 in the channel gain list - I suspect the latter is the case, but right now I'm coding the former

- add NVS's one pixel row per channel display window, with greyscale of colour hue indicating voltage - see more recognizable patterns, can see spikes over the span of just a few pixels (look like gabors over space and time), yet can also look across a wide swath of time to find similar looking gabors elsewhere - interesting

- Nick's suggestion: try smoothing across channels. For each timepoint, for each chan, weight the signal at that timepoint with 1, and all the others for other channels as a function of a gaussian. Then normalize to get the smoothed signal value for that channel for that timepoint. Then, repeat, centered on each of the channels, and repeat for all timepoints. This preserves signal they have in common, and gets rid of channel-independent noise. Drawback is that you lose spatial resolution for your spike: a spike will appear to spread across more channels than it did before smoothing. This tradeoff can be tweaked by changing the sigma of the 2D spatial smotthing gaussian. Looks pretty good in his implementation.
    - might be a bit slow, but could just be implemented as an extra step before or after interpolation. is equivalent to convolving with some kind of spatial filter? maybe this is already implemented somewhere in numpy or scipy

- for convenience in neuropy, copy all stimulus data from .srf to .sort file

- think there's an issue with the cursor width not updating when the window it represents (the one to the left) changes its zoom level

- add CSD calculation and display

- get tooltips to work on main spyke frame widgets

- make ESC in tree and event list clear the current selection

- when assigning an event to a plot, change the zorder of the lines such that the maxchan is always on top, only really need to deal with the maxchan, which can sometimes be big enough to span over to the screen real estate of other chans
    - can make this a template method too, such that it sets the zorder for all lines in constituent events to be the same as it is for the template mean

- hovering over waveforms should highlight the closest datapoint on the closest chan with a circle point or something, while displaying its time and exact voltage in the tooltip - this would probably be more useful than displaying the voltage in the potentially empty space under the mouse
    - should do this both for spike window and sort window (not really necessary for chart or lfp, might be slow, but might be nice too), although in sort window it wouldn't be clear which waveform to do it on, for overlapping waveforms

- plot and control thresh level on spike and chart plots
    - use picker - when a thresh line is picked (within a tolerance of a couple pix), change mouse cursor to hand or vertical sizer, and while button is down, adjust position of thresh lines and value of the manual thresh level in Events tab

- separate groups of 3 digits in spin edit and end labels with commas (time in us)

- add 1ms wide chart window to sort window, so you can see chan layout of events in both real space and chart space, might make it easier to sort

- combined message and stimulus DIN window:
    - messages: maybe just a combo box with timestamp:message entries. Choosing one of them skips you to that timestamp. Changing current file position with other methods should automatically scroll you through the combo box at appropriate times
    - stimulus din: similar to above: drop down combo box with timestamp:DIN value entries. Again, changing current file position with other methods should automatically scroll you through the combo box at appropriate times

- stop forcing the main window to position itself at (0, 0), check what the upper leftmost available position is - for someone like Nick with the taskbar along the top of the screen, it's more like (0, 30)

- this might speed up plotting even further: have only one actual Plot object per plot panel (the quickRemovePlot), and then another that's just a shell for temporarily holding plot attribs (data, colour, style) which are updated, drawn to buffer, updated, drawn to buffer, etc. and then blitted (this is actually a lot like having axes.hold = True)
    - I think this would vastly reduce the number of Line2D objects needed - initing them takes a long time, and once they've been inited, they sit there taking up memory
    - not sure how this would affect used_plots and available_plots
    - seems like a good solution for overplotting thousands of plots in the error threshold window
        - or maybe better thing there would be to only overplot on top of the template mean say the 10 or 20 events with the highest errors that fall below the current error threshold

------------------------------------
INTERPOLATION

    - when resamplex > 1 and shcorrect == False, you only need resamplex - 1 kernels. You don't need a kernel for the original raw data points. Those won't be shifted, so you can just interleave appropriately

    - take DIN channel into account, might need to shift all highpass chans by 1us, see line 2412 in SurfBawdMain.pas

    - could use speed improvement

    - only bother displaying interpolated data for spike frame - not necessary for chart and lfp frames, and will conserve update speed to a large extent compared to uninterpolated mode

------------------------------------
EVENT DETECTION:

    - LOCKOUT: do I propagate any spatiotemporal lockouts from the end of one searched block to the start of the next?  I'm taking slightly overlapping blocks of data to make sure I don't miss any spikes that fall on the borders, but I'm not sure if I'm propagating lockouts...
        - pretty sure the answer is no right now

    - maybe instead of locking out, I should subtract the modelled waveform from the raw data, and allow the event loop to test all its detected events
        - this might help in dealing with overlapping spikes
        - maybe I should retest for new events in the subtracted data that might come out
        - somehow make sure that when subtracting off the model from the raw data, that you're always bringing the raw data values closer to zero, no matter what. Have to do some abs()-ing somewhere, I suspect, or at least some checking of values > 0 or < 0

    SPIKE MODELLING:

        SHORT TERM:
            - check to make sure that if model gives you a single phase spike (one of the phases is pretty much 0 amplitude), that you don't dismiss it as a false event, just because the two phases (negligibly) have the same sign in their amplitudes - ie, don't mistake what might be essentially single phase spikes for double phase spikes with the same sign (up up, or down down)
            - check modelled y0 - if outside the y coords of given chans, throw the spike out - this is probably better than putting constraints on the modelled y0
            - is x0 unfairly weighted according to how many chans each col has? eg, if left col has more chans, is x0 biased to the left?
            - store param names in a list of param strings sorted the same way as values in sm.p, override SpikeModel.__getattr__, check if attr is in the param string list, use its index to return the value in sm.p. That way you can go sm.mu1 without having to worry about param order changing in the future
            - log detection and modelling printouts to a file, search through that instead of having to search through lousy winxp terminal


        - except for the initial threshold test, instead of directly checking raw voltage values to see if they exceed certain values and when that occurs (like a window discriminator), how about this:
            - when you find a spot that exceeds the initial threshold, fit a spike model to the signal on that channel and each of the surrounding channels within slock. The spike model will be biphasic (or maybe triphasic). Now, of your modelled channels, find the one with the maxchan at the peak of the initial phase, model any further channels that might fall within the slock of that new maxchan, and search one last time for the maxchan within slock
            - model parameters: overall sign (+ve sign would be say a normal initially downward going spike), overall width (up to 1.5ms say), and amplitude of each phase (phases will be forced to alternate in sign)
            - once you've found your best fit model (which will necessarily be smooth), use it to decide if it qualifies as a spike (ie, use it to check amplitude of its second phase), and if so, to decide how long to lock out for
            - this would be a kind of denoising, and would get around weird cases where the spike is distorted, which if you look at raw values, screws up your decision of where the peaks are and how long to lock out for
                - this would probably get rid of the spike alignment problems when matching templates
            - should the model be a Gaussian windowed polynomial of order nphases + 1?
            - how best to go about actually fitting the model? what's the numpy/scipy algorithm for this?
                - there's scipy.optimize, which has a .leastsq() f'n
                - there's also matplotlib.mlab.polyfit
            - then, if you've decided it qualifies as a spike, you can save the model for it, and use that for template building and matching
        - Nick says: Levenberg-Marquadt algorithm (LMA) is least squares, and so is simplex? What differs is just the cost f'n?
            - ah, scipy.leastsq is LMA
            - Nick: instead of doing template matching after modelling the spike, you can just plot the model parameters in a multidimensional space and do normal clustering - skip the whole matching step!
            - says best to just use sum of Gaussians, no need to multiply a N+1 order polynomial with N gaussians
                - tried this doing some manual fitting, sum of Guassians looks great, and is very malleable to get you the spike shape you want
            - also, in the same go, you can model the position of the spike in space - just add a 2D gaussian model for space (that's another 4 params: two means + two stdevs) - then, when you cluster, you're in a very intuitive space (even if its, say 6+4 = 10 dims) that involves the means, stdevs, and amplitudes of two spike phases, plus the position and spread in space.
                - this brings us all the way back to what was Phil's original idea: use physical space as your clustering space
            - should actually have another param theta for angle of 2D gaussian
        - I should probably try this all out in pure Python to start, including the intial threshold detection step (using dynamic thresholds)
            - nothing in this whole modelling method precludes changing anything about the spatiotemporal lockout - that can stay as is. What changes is that the lockout becomes more accurate, since you're modelling the spike peaks in both space and time, which is less affected by spatiotemporal noise than the raw data values
            - for initial spike detection, instead of just looking back one data point to see if signal is diverging from 0 (which itself is susceptible to noise), maybe look back two or more points, or average across the last two or more points to make that decision - this would make you less susceptible to suspect a spike due to a little mini up-down event on the down slope of the final phase of the previously detected spike
                - or, if you see an up-down event, check if it exists on other chans at the same time - although, this assumes that the spike is multichannel, which it isn't necessarily
            - might also be able to reduce num params by fixing all spike phase gaussian means and stdevs to be the same (assuming S+H correction is turned on, and there aren't any backpropagating spikes)
        - spike at ptc15.87.23700, chan=3 could definitely use separate vertical and horizontal spatial sigmas - the vertical one would be greater
            - same goes for spike at ptc15.87.21940, chan=10. Maybe sigma vector amplitude and angle would be better
            - having two spatial sigmas probably makes the problem underconstrained for a 2 column probe, but it might work on a 3 column probe - I think Tim concluded this as well
            - problem is that to compensate for something that obviously isn't circularly symmetric in space, it sets the source way off the probe to the left or the right. This then messes with the spatial lockout, which is centered on the modeled source location, so you end up getting double triggers.

        - another strategy might be to give up on the spatial modelling, fix temporal gaussian means and sigmas across chans, but give each chan its own two amplitudes, then just triangulate to find spatial mean, or then run the model in a separate independent run, using the modelled output values in time from the previous run, to decide on the spatial means and sigmas and orientation. This will help prevent some of the parameters from confusing the others. In other words, maybe the first run should be concerned with finding the spike's point in time, and the second run with finding its point in space.
            - tried this, didn't help under LM. Not sure if I tried it again under R-alg. Anyway, it was complicated and confusing, and really, you should try to fit everything at once

        - maybe I should do the full 3D modelling that Tim did, this would only require adding one extra parameter: z0. sigmaz would be the same as sigmax. Also, maybe theta should just be plugged into the model as a constant, determined by the user, and defaulting to 0
            - might also try a 3D Guassian instead of the 3D 1/r. Gaussian is smoother and well defined around 0, leastsq might be more stable with it
            - thought I tried this, and model wasn't converging well. Maybe this was for the Levenburg-Marquadt, before I tried the R-alg.

        - could also try feeding just the main peak or the two peak amplitudes of each channel to the spatial model, instead of all the points in the spike. The peak values might be more consistent with the spatial model, and therefore easier to fit and less likely to result in ridiculous spatial parameter outputs
            - benefit: treats normal and backprop APs equally - otherwise a backprop
            - or maybe just fit the biggest abs(peak) instead of both peak and valley, since the two sometimes give conflicting info -- one chan might be max for the first peak, another will be max for the second - see ptc15.87.28880
            - or maybe model both peaks, but in separate runs, and use the results of both runs for clustering

        - better way of dealing with backprop rather than just taking the peak amplitudes, is to leave it full waveform and add an AP velocity parameter (um/us)
            - could assume that propogation delay is up (or along the user-supplied theta), or could have two different delays, one in the x and one in the y direction (probably unnecessary). Either way, the current spike time at the x, y coords of the spatial model would be the 0-point time reference

        - rotate probe xy axes slightly (say by 10 deg in some direction), but leave the origin at the center top of the probe - this change of coords would give each chan a unique value for both x' and y', and all chans would contribute equally to both x' and y' related parameter tuning - currently, the chans in the same col all have the same x values, and among themselves they tell you nothing about x related params (position, sigma, or velocity)
            - Nick doesn't think this would help - besides this is redundant with providing a spatial theta for each spatial Gaussian envelope to be rotated by

        - interpolating to 100 kHz would double the number of data points the model could work with - this might make it perform better?
            - interpolation is very fast compared to the R-algorithm, so this might be worth trying

    - could speed things up by only interpolating say each 1ms of data following an initial threshold crossing, maybe interpolate to 100 kHz while we're at it, and leave everything else at 25 kHz without any S+H correction - you don't really need interpolation or S+H correction for the initial threshold detection, I think...

    - create a new thread or process for each searchblock() call

    - dynamic spatiotemporal lockout: some spikes span greater space and time than others, yet they're all given the same lockout. I guess on thresh xing and again on finding a spike, you could search locally in time and space and within the fixed slock and tlock, and see if signal drops below thresh in something less than slock and tlock, and adjust the lockout accordingly for that thresh xing or found spike

    - doubleclick detection_list row brings up textctrl in a (modeless?) dialog with entire 2D events array for that item

    - change event array returned by detector.search() to be many rows, 2 cols, instead of 2 rows, many cols. Would be more natural that way:
        - printouts, no matter how long, would always have maxchan and timepoint right next to each other
        - length checking (of number of events) would only need len, which works on rows

    - maybe consider looking for some kind of multichannel threshold crossing instead of testing each channel separately at the start of the inner loop: if say sum or sumsquared of all chans exceeds some (potentially dynamic) threshold (say 3.5*median for each chan * number of chans), then maybe you've stumbled across some event that's hard to see on a single chan basis, but is more obvious if you squint your eyes and look across a few chans at once (see ptc15.87.102000520, chans=[30,22,31], most easily visible in chart window)
        - or maybe for each timepoint, grab triplets (or quads or something) of neighbouring channels and look at their sum, instead of the sum for all chans, but then for each timepoint you'd have to do this for all possible neighbouring triplets, which might start getting slow
        - this will help find small spikes that might otherwise be missed, but fixing false negatives is less important than reducing false positives

    - Nick: find fit errors between each modelled spike and its raw data, apply some threshold for fit error (normalized by number of chans? or by mean voltage across all chans), reject modelled spikes as invalid if their fit error exceeds your threshold - threshold might be same as or based on noise threshold used for initial event detection?

------------------------------------
CLUSTERING:

    - should probably stop normalizing x0 and y0, even if you then reweight them somehow yourself, because normalizing destroys the physical distance metric between spikes

    - I want clustering to first consider x0 and y0 position exclusively, and only after the fact, use all the other spike model params to further divy up the spatial clusters. Throwing all the params into a big multidimensional dataset and clustering it all in one go won't accomplish this, because there's nothing sequential about the consideration of the parameters. Some can have more influence than others according to how you weight them, but you can't specify sequential treatment of params. The result is that often you get clusters that don't correspond particularly well to their x0 y0 values, even after manual reweighting. Instead, they correspond to the other param values
        - the idea is that the secondary params should only be there to refine the x0 y0 clusters
        - I think the only solution is to do multiple cluster runs, with different param sets in each. First, cluster using only x0 and y0. This could result in undersplit clusters, but shouldn't have any obvious errors such that a spike from the top of the probe is clustered with one from the bottom, for example. Next, cluster all the spikes again, but use only the other params. You now have two sets of clusters. The second set will probably have fewer significant clusters in them, and could be useful for stuff other than spike sorting - ie, looking at all cells that have similar spatial sigmas, or similar thetas, or something. Now, take the intersection of the two sets of clusters somehow. Some set() method should be able to do this. This way, you potentially break down a cluster defined by x0 y0 into subclusters defined by all the other params. There could also be more than just two sets of clusters
        - to get intersection of two cluster sets, take first cluster in first set, find intersection of it with each cluster in the second set. Each non-empty intersection becomes a new cluster (or subcluster if you like). Repeat for all clusters in the first set.

    - PCA and ICA should be tried at some point during clustering - these wouldn't automatically solve my sequential clustering desires, would they? Maybe for the second set of clustering, I should just throw all the remaining params into a single multidimensional dataset, run PCA/ICA on it, pick the few most significant components, and work on those
        - use mdp package for PCA and ICA
        - note there's also a py_ica module that does infomax ICA

    - perhaps SPC doesn't work very well with a small example dataset of only a few dozen (82) spikes because it uses neighbourhood density to cluster, and in this case, density is quite low everywhere...
        - lowering Knearestneighbours (!=q) seems to increase the number of non-unitary clusters it's willing to produce

    - Nick: work on way more spikes, few thousand, and evaluate using similarity matrix, forget about checking each and every spike manually. Also suggests to try PCA/ICA first to help you throw out the not so useful params
        - similarity matrix between all spikes would probably be of modelled spikes in voltage space (single trace per spike) - would hope that the error between modelled voltage traces within a cluster is much less than that between clusters
            - actually, I'm not so sure about doing this in voltage space, since that can be somewhat variable - modelling it with params abstracts out that variability, that's the point - should probably do similarity matrix in parameter space instead, or something

    - read through Fee 1996 paper a bit, got idea that, if I do hierarchical clustering, I should try a really low T threshold, and then increasingly bigger ones, and at each resulting hierarchy flattening, check to see which of the clusters at the previous level have agglomerated, and for those that have, check if any of the agglomerated clusters violate single-unit stats (see Fee 1996). If so, should probably go no further than the previous T level
        - Fee 1996 also says to reject clusters that are multiunit, according to R2/10 = 8.8/0.8 * F(2)/F(10) - some kind of ratio of ISI distribs

------------------------------------
TEMPLATES:

    - Nick: add kmeans to quickly generate templates
        - maybe not so hard, since kmeans is built into scipy I think, but it's still going to require arbitrary parameter values that you'll have to set, and it might give you a false sense of completeness/correctness

    - replace event listctrl with a virtual listctrl, for faster populating of (and scrolling through?) many thousands of detected events

    - unsorted event list text background should be coloured according to maxchan
        - second list control below it for trash?

    - hitting ENTER on a single event in event list should seek the dataframes to that timepoint (just as clicking on the event in the sort window does the same, sort of by accident) - right now it toggles selection of currently focused item, as does SPACE

    - subtemplates - recognize that two distinct events might be from the same cell, but in a different mode of operation
    - toggle the display of all the templates (their means) at once (button, or Ctrl-A?), with just their active channels displayed. This way, you can quickly see if some templates are very similar, and how the cells are distributed across the electrode. This also lets you decide which templates need more/fewer channels enabled to help distinguish between them. Each template should be a different colour (up to say 16 or so).


    - MANUAL SPIKE ALIGNMENT/DURATION:

        - activation (double-click/enter) of either an event list item or a tree item will bring up a dialog box (one that doesn't prevent access to its parent while it's open. modeless?) with two spin ctrls which let you set the trange of the event/template. Plots should update dynamically while the dialog is still open.
            - modifying a template's trange should do the same for all its member events
            - modifying the trange of a template's member event should also modify the template's trange and that of all other member events
            - for events, add a 3rd spin ctrl to modify the events .t timestamp, for aligning misaligned spikes (triggered off the wrong phase, etc.). Again, should have this dynamically update the event's plot on every spin ctrl event
                - every time you modify the timestamp to your liking, maybe when you hit OK it should check to make sure you haven't duplicated some other existing event with exactly the same maxchan and timestamp. This can happen if two events which are really one and the same are detected by the detector, and you modify one to match the phase of the other

        - do this on a per spike basis as a feature, if automated alignment can't always get it right
        - when doing alignment on a template, move all member events too. Duh. We'll just be adjusting tref for the template, which should then trigger a re-cut of all member spikes?

        - custom spike duration per template
            - Need to make the spike duration (currently 1ms) user settable for every template. Some spikes are broad, others narrow, and it's best to confine the templates to just those times with useful information about the spike. Otherwise, if there's too much flatline on either side, then you're basically saying "it ain't a spike unless stuff doesn't happen immediately before or after it", which isn't true. You might get a spike very soon after from another cell, or even from the same cell if it's a fast spiker.
            - in other words, we want the ability to crop a template in time. This complements the ability to crop a template in channel space

    - for Sort frame, catch Paint event or whatever so that instead of doing a full redraw after de-occlusion, you just restore the saved .background

    - TODO: yet another wxTreeCtrl | wxTreeMultiple flag bug to report to trac: when moving down from the last tree item, you lose highlight (but not focus). Then when you press up, focus and higlight jump to second last item. Similar situation at the top of the tree.

    - when a combination of templates and events are selected in the tree and you hit Delete, leave the templates selected, and only delete the events
        - this should then update the plots for the templates, because their member spikes have just changed

    - cycle between next/previous set of 10 or so member events for current template (or first selected template) on > and < keypresses. Or maybe + and - keypresses

    - hover over event in tree should give you tooltip with event time and maxchan

    - BUG: after opening a .sort file, if your first click is on the first template, no selection event happens, and template mean isn't plotted. Have to move to another one first, then back. Not a problem if first click is on something other than first template
        - smells like another wxWidgets bug



------------------------------------
MINI MATCH/RIP:

    - while building up templates, have ability to do sort of a mini rip of your current templates across the current subset of spikes.
    - this would allow you to quickly increase the number of spikes in each template, and would reveal which spikes in your subset are still very different from any templates you've generated so far, so you can prioritize and focus on those next
    - more importantly, this will aid in sweeping up spikes from those cells that fire a lot and distract you from those that fire rarely
    - a degenerate version of this would be to do a self rip, where you rip across only the set of spikes that are currently members of the template. This would allow you to adjust the error thresh until the self rip just barely returns all of the current members of the template
    - just as for a normal rip, you'll first need to set an error thresh for the template
    - as you progress in your template building, you can occasionally do a mini rip and look at the error histogram, to see how well that template separates spikes out from the subset. Should be able to adjust the error thresh such that you get exactly the spikes that are already members

'''
------------------------------------
MATCHING/RIPPING:

    - instead of ripping against all the raw data, spike detect the whole file, and then rip against the detected spikes. If you center both your templates and your events on the peak, then comparing a template to an event is a one-shot process - no timeshifting required. 2 benefits:
        - much faster
        - error histograms will be more sharply bimodal, since you're not comparing templates to noise ever, just to events
            - also, no longer have to worry so much about selecting as few flat chans as possible in your template, in the worry that it'll start triggering against flatish lines of noise in the raw data
    - to reduce spike variability due to correlated noise, for any given comparison between template and data (be it against any timepoint in data, or against a detected event), take the channels that aren't selected in the template (purportedly because they have no significant signal on them), and at every timepoint, take their mean, and subtract that from _all_ channels. This should make spikes in the raw data (or in the detected events) look more like one of the templates
        - mean might not be ideal - maybe take their covariance and subtract it or something (see hyperellipsoidal method ref'd in 200x Blanche)
        - this should be done during any comparison between template and data, including during min rips (see below)
'''

------------------------------------
VERIFICATION:

    - after completing a rip of the whole file, assemble the spike times from all the templates into a single list. Compare it to the list of spike times generated by applying the spike detection algorithm to the whole file. See how the two sets of spike times overlap. Imagine a venn diagram: if there are spikes that the detection algorithm found that the MTM algorithm did not find, then you need to examine those spikes. Either:
        A) you set your error thresh too stringently for the template that should have matched that spike;
        B) you have bad templates, or you're missing templates

    - plot autocorrelograms for each template, and cross-correlograms for all template pairs, make sure the bin heights are ~0 for abs(deltaT) < 1ms for autocorrs, make sure you don't have ridiculously huge peaks in xcorrs
        - wasn't there some other related test that we should be doing?
    - Nick: create a distance/similarity metric that, for all pairs of templates, for all overlapping channels, measures the difference between the signals, normalized by, say, the number of overlapping channels and the number of points per channel. This would help confirm that two templates are indeed two separate cells

    - plot spikes as fat bright vertical lines or something underneath the chart window so you can scroll through and see just what was classified as a spike and what wasn't. Vertical line colour should match template colour

    - limit template generation to specific part of file
        - Nick's idea: randomly sample events from, say, only the first 10 minutes of a recording, build up your templates with those. Then, rip across the whole file, and watch if fit gets worse the further away you get in time.
            - plot goodness of fit over time - Goodness of fit might change over time, especially over hours, and in recordings different from which the templates were generated. Either divide each recording up into pieces say 10 minutes long, and plot that, or have a sliding window that calculates the goodness of fit for the last 10 minutes (that would give you lots more data points)


------------------------------------
LOW-PRIORITY:

- add mouse-controlled time range selection to slider

- CTRL-B to bookmark a position (plot a red vline at that timepoint in all data frames? and/or add that timepoint to file position drop-down combo box), CTRL-F3 and CTRL-F2 to skip forward/back to next/previous bookmark (or select it from list of bookmarked times in file pos combo box)
    - good for skipping back and forth quickly between two (or more) spikes or otherwise interesting regions

- add file pos spin button, and catch the spin button's events (see wx manual pdf page 240) to step through file
    - or just use pgup/pgdown or slider paging

- add scroll wheel support for data frames, for scrolling through data
    - scroll by 40us in spike frame, 1ms in chart frame, 50ms in lfp frame
        - add a .tres attrib to each data frame?

- add vertical zoom controls to all DataFrames

- toggle between garish rainbow colours and normal green

- toggle plot points

- make moving of main spyke window retain existing spatial relations to data frames, instead of resetting to original spacing
    - worked on this for quite a while, couldn't figure it out, had recursive calling problems and stuff
    - ah, I was looking for something like this for a long time: wx.PostEvent(window, event) lets you send any event to any given window. Should try this out...

- goto feature: CTRL+G brings file position spin control into focus where you can enter the time that you want to jump to
    - maybe add ability to choose units (us or ms or s) from a combo box

- parsing progress bar with pause/stop button
    - use threading, so widget draw events aren't blocked during parsing

- load progress bar

- save progress bar

- event detection progress bar with pause/stop button

- matching/ripping progress bar with pause/stop button






------------------------------------
OLDER:

- move simple matplotlib plot() method from Stream class to WaveForm class, that way, you no longer need to pass a trange argument
    - instead of:
        f.highpassstream.plot(chanis=None, trange=(135e6, 135.1e6))
      you'd slice instead:
        f.highpassstream[135e6:135.1e6].plot(chanis=None)
    - would have to add more stuff as attribs to WaveForm class, such as .records and .rts

- spike (and lfp? hardly worth it?) data need to be corrected for sample and hold (S+H) delay offsets
    - do this during interpolation as in Surfbawd?

- LFP data need to be corrected for offsets induced by low-pass filters
    - what are the spectral phase and amplitude characteristics of the filters?
    - Hilbert transforms are useful for this?
    - want to do CSDs as well - I guess these don't need to be filter offset corrected, since you're just comparing between lfp chans, not between lfp and spike chans

- could eventually add monopole and dipole modelling for 3d localization and to distinguish between pyramidal and stellate cells

- to deal with spike adaptation and attenuation during bursts, for each template, rip not just its mean across the file, but also the biggest and smallest (in terms of something like sum of peak-to-peak voltage across template channels, or variance across channels, or...) spike assigned to that template.
    - or, for each template, plot the distribution of spike sizes for all spikes in that template. For some templates, you might get a skewed distrib in which case: A) you've got a polluted template that needs to be split
                                    B) you need to do something, can't remember what I was gonna write, maybe increase the error thresh for that template?
    - maybe have ability to go from bins in spike size histogram (say its tail, or some weird peak) back to a display of the individual spike(s) that contributed to that bin


- need to be able to select which channels to enable in the template before doing a rip
    - for each template, use mouse controlled bounding box/clicking to pick out significant channels
    - if there's another template nearby in channel space, it's good to also select a few surrounding flat channels to help distinguish the fits of the two templates during the rip

- hard to do?: would be good to have undo/redo for all the template membership manipulations

- what kind of window should we have around the spike time? Does the spike time represent the time of absolute value of peak voltage on the channel with the biggest signal? Then the temporal window around that is -0.25 ms and +0.75 ms for a total of 1 ms window? I think that's roughly the standard in surfbawd. Temporal window parameters (-ve and +ve offset from spike time) should be a global setting that everything uses, but one that you can change at will (at least when spyke isn't currently doing a rip) to accomodate the display of the occasional long slow spike

- do we want to allow templates to be generated from spikes from multiple recordings? this could add annoying complexity, but might be handy for getting enough spikes from those rarely firing neurons - some neurons might fire only during one stimulus type and not the other. Neurons might have opposing preferences, such that you never see them all fire within the same recording

- low probability: in case of electrode drift within a recording, allow user to make executive decision that two templates that are significantly different in voltage+channel space due to drift, yet sufficiently similar to the eye, that they should be assigned the same neuron id (keep their template ids separate?)
    - should there be a distinction between template and neuron ids just in case we want to do something like the above? Maybe the template id should become alphanumeric, with a number followed by a letter, say 12a, 12b, etc.

- OLD Cython event detection: for each searchblock() call, I only allocate enough memory to store event times for 54 cells (one per chan) with an average firing rate of 1 kHz - perhaps I should check in the cython code if I'm about to exceed the bounds of the eventtimes array, and grow it when need be, or at least print an error, instead of segfaulting!


----------------------------------------------------
DONE:

- move code to /spyke subfolder, make a setup.py in root, keep TODO in root
- how hard will it be to generate highpass, lowpass, chartwindow, and fistogram widgets in wxpython?
    - email wxpython list on displaying time series data.
    - wxpy 2.8 should be faster, doesn't need to copy data, can use numpy data directly?
    - borrow something from MPL?
- need ability to take a spike that's assigned to an existing template and remove it and generate a new template from it
- NOPE: destory data windows instead of hiding them - should be simpler in the end
    - set up config system to, among other things, remember window positions and sizes when you re-open a window, whether within the same session or across sessions
- try making data frames MiniFrames (see manual) instead of passing them the TOOL_WINDOW flag
    - consider making data frames children of the main spyke frame again - maybe this could be done to automatically iconize and activate the data frames when main spyke frame gets the event - yup, this is exactly the case. hooray!
- chart window channel layout is wrong
- arrange data windows so they overlap as little as possible
- end range problem for chart window, allow plotting of nothing...
- remove hard coding in data window positioning
- have a dock mode, where moving one window, whether main spyke frame or any of the data frames, moves all windows the same amount
    - I think all you need to do is give all frames an OnMove method (or something), call all other windows' OnMoves, pass them the event, and then event.Skip()
    - or better yet, make moving the main spyke window always move in dock mode, and moving the dataframes not do so (er, this last one seems hard to do, leave it out)
- make seek move chartwin's center to that position
    - that way you can look at stuff before and after the spike win data
    - also, a centered seek will allow for natural centered zooming in and out in time
- for laying out channels in space, should really be doing a um to uV conversion vertically, and a um to us conversion horizontally. Instead, right now I'm just maintaining vertical and horizontal order, and assuming all chans are equally spaced within those ordered lists
- shade the 1ms (or whatever the spike frame width is) on the chart window with a slightly lighter background than pure black, to indicate what part of the chartwin data you're currently looking at in the spike window
- make spike frame use same colours and chan to colour mapping as chart frame - chart frame is rainbow, spike frame will be similar, but not exactly the same for probes with chans that line up horizontally (like the 3 col probes)
- make spike frame width depend on number of columns in probe, so you get consistent channel width for all probes
- replace default slider position text (which keeps corrupting anyway) with proper spin edit and dedicated text boxes to indicate current position and start and end of recording
- nah: add chan id toggle button to display chan id directly underplotting (or shown next to?) each channel in all the dataframes
    - instead: show timestamp and chan id tooltip on hover
- add faint 0 uV horizontal lines for spike and LFP windows (not necessary for chart win)
    - add faint vertical lines at center of each column in spike window
- add horizontal zoom controls to all DataFrames
- when searching for maxchan, limit search to chans within spatial lockout
      - prevents unrelated distant cells firing at the same time from triggering each other
- add ability to search backwards in time:
    - maybe just slice data in reverse order, Cy loop won't know the difference?
    - would also have to reverse order of block loop, can prolly do this by
      simply making blocksize BS -ve
- BUG: searching for next/last spike with only some channels enabled does weird multiple triggering off of bigger spikes on that channel, and sometimes segfaults if F3/F2 is held down
    - doing the same using search button alternates between two results when some chans are disabled
    - was due to a mistake in indexing into distance matrix in cython code
- create a win32 package installer, or just do python setup.py install, don't really need to bother with the whole py2exe thing, just manually install python, numpy, scipy, wxpy, and mpl on clients
- deal with case where you've got just a .sort file open, and you try adding an event with no wave.data to a template. Prevent user from doing this! Right now it removes the event from the event list and gets as far as trying to update the template mean, at which point it runs into an AssertionError. Need to handle this properly
    - maybe you shouldn't even be able to select an event when its wave.data is unattainable...
- be able to specify time range(s?) within a file to limit spike detection to - this could be useful for limiting detection and template generation to parts of the file during different levels of animal wakefulness, as indicated by the dominant frequencies in the LFP
- specify chans that you want to search for events on. 2 benefits:
    - helps you increase n for a template that rarely fires
    - say you notice a shortage of cells in a certain part of the probe - you want to be sure there aren't any cells there that you've missed
- F3 and F2 to search forward and back one event
- toggle chan selection by Ctrl+click on chan in any of the data frames - this either makes them invisible (but that could cause problems for tooltip when it's looking for the nearest line) or toggles their colour to dark grey or something.
- have a log window at bottom that details everything going on behind the scenes
    - this could simply be an integrated pyshell window (see manual), so you can do things from command line instead of being forced to use the GUI
    - also, use it to examine and mess with internals, including wxpy internals
- keep spyke importable as a library, not just as a standalone program. This way, you can parse files, spike sort, etc directly from interpreter, or neuropy, or any other python project.
- Template.chans should be a property. Every time the user changes the channel selection for a template, you should do Template.chans = Template.get_maxchan() to get the new maxchan from the new set of enabled chans
- introduce a weighted error per template chan, where the template's maxchan is weighted the most and surrounding chans are weighted less as you get further away, maybe a 2D gaussian distance weighting function with standard deviation of one spatial lockout radius
    - use a 2D matrix?, generate it from the actual distances between channels in the probe layout?
    - could also weight points with a gaussian in time, centered on t=0, with stdev=tlock
- Detection deletion:
    - allow if none of the Detector's events have a non-None .template attrib
    - if we allow deletion, maybe session.detections should be a dict instead of a list
- need to handle OnResize in DataFrames like I do in the SortFrame - background needs to be regenerated on every resize event
- in detect_cy.pyx, get_maxchan and set_lockout both have awkwardly long arg lists, maybe make up a struct type whose fields point to all the variables used in this method, and pass the struct to get_maxchan and set_lockout
    - or, just make most of the variables globals, using global keyword
- change lockp from being a counter to holding the absolute timepoint until which the chan is locked out, that way you don't have to decr.
- waveform widgets should display data with the current level of interpolation (surfbawd only ever displayed raw data)
- if matplotlib is used for waveform widgets, will get subpixel rendering and antialiasing for free, which should make interpolated data visibly different from raw data even at low zoom
- randomly sample the srf file to get some (hopefully) representative subset of all the multichannel spike waveforms
- static and dynamic thresholds, independent for each chan, based on stdev or median (or other measures?)
- recreate Detector with current setting whenever user clicks on method or threshold radios
    - clicking on other options can just update the current Detector
    - or better, Detector is updated from other options only when you hit search or F3 or eq'v
- unsorted event list control should have sortable columns: id, maxchan, timestamp.
- use .GetTopLevelParent() from (any?) widget to get a reference to spykeframe, safer than doing a bunch of Parent and GrandParent stuff
    - actually, that only seems to return the frame the widget is in, but then from there you can take the Parent to get tye spykeframe
- tabbing between tree and list ctrl works, but there's something sort of in between that the tab lands on, yet after landing on that something, if you hit up/down, you realize you're actually focused on the event ctrl, just can't tell cuz the current selection for the event list ctrl doesn't highlight where you're in that twilight focus zone
    - i think the twilight zone is actually one or both of the sort panels, cuz the keydown printout happens when you hit a character in the twilight zone
    - MoveAfterInTabOrder and MoveBeforeInTabOrder should help
    - OK, now I've figured it out. It's the stupid .plot_pane that's getting the focus. By deleting the plot_pane, I get rid of the problem. So the trick will be to enforce that the plot_pane can't get focus. Normally, you'd override AcceptsFocusFromKeyboard, but that doesn't work for C++ classes, so you have to do something else. See: http://lists.wxwidgets.org/pipermail/wxpython-users/2008-April/074292.html
    - captured tab keypress in both tree and list controls, use them to transfer focus to the other control
- BUG: splitterwindow doesn't actually set itself to desired size until after you move the mouse for the first time
- make SPACE alone, without CTRL, toggle selection of currently focused item - DONE, but there's an annoying flicker due to the hack around a wx bug, probably can't be eliminated til the wx bug is fixed
- Nick's suggestion: add ability to sort event list by match error, as an alternative to sorting spatially by maxchan. Ie, do a mini-match between the currently selected template or event in the tree (make sure only one item is selected) and all the unsorted events in the event list, and sort the event list by their err. Maybe add an err column to the event list that gets filled in when you ask for a mini-match, and then click the column header to sort by err. Note that many of the entries in the err column will be blank because many events won't have enough similarity in overlapping channels/maxchan distance away to merit even generating an err value.
- try parsing by restoring from .parse files again for speed, especially when it comes to testing...
- rename Session object to Sort object
    - shorter
    - corresponds better to .sort file
    - corresponds better with Sort level (proposed renaming of Rip) in neuropy object hierarchy
- transparency would be good for overplots
- neuron ids should be incrementing integers. When you delete a template, the following templates' ids and colours should remain unchanged, until you hit a toolbar button (or something) to indicate that you want to renumber (and recolour) the whole lot, which will effectively only renumber and recolour those templates following the one that was deleted
- template files
    - contain python syntax that can be eval'd, or config file format, to get you everything you might ever want to know about the template:
        - file (files?) it was generated from
        - member spike times (and their associated files?)
        - date time it was last modified
        - ...
    - store all templates in one file ala surfbawd? or one file per template?
    - store it all in a single SQL database file?
    - THIS ONE: or, pickle the whole Sort class instance to file, but also make it possible to export to separate binary spike files (plus the binary .din file)
- are there really two different record flags (M and MS) that both indicate a MessageRecord (see surf.py)? What .srf files do either (or both) occur in? Which one is more common? The MS one?
    - no, they're MS and MU, for Surf and User generated messages. At least, there shouldn't be any .srf files out there that have only M as the record flag

- instead of simply searching for max and min V within window, search in window forward from thresh for a peak, then in either from that peak for up to another 200us for another one of opposite sign. If you don't find a 2nd one, it ain't a spike. How to find peaks? Look for change in diff? Nah, use inline C.
- profile pickling and unpickling of both .parse and .sort files by temporarily using the Python pickle module instead of the cPickle module, to hopefully figure out why the surf.File and sort.Sort objects take so damned long to process - too many references?
- assigning 64 bit range values to the wx slider on startup overflows if they're outside 32 bit values - this is a problem for 7 GB .srf files. Might have to use the slider as a rougher metric - instead of us, have it represent 10us steps? Or interpolated Xus steps? This then needs translation between slider values and actual time values...
- abstract out spike detection and obvious rejection from spike modelling, do all detection first in its own loop, followed by all modelling later in its own loop
- Nick's suggestion: consider leaving voltages in AD units, as signed 16 bit integers (centered around 0 instead of 2048 as in the .srf files) instead of float32's. This would use half the memory, and might also speed up computation: integer math should be faster than float math. Whenever you actually need uV, just convert on the fly
    - I think I decided to stick to float32's because during Nyquist interpolation, I have to switch to them anyways, and I'm always doing interpolation
- does disabling a channel really prevent it from being searched on? yes
- just a note: user should probably disable any grounded out chans in the dataframes, before beginning detection/sorting/matching. Any disabled chans in the dataframes should propagate through the whole process as disabled by default (not sure if this is the case right now)
- when searching for maxchan, new one should exceed current in Vpp, not just in Vp at phase1t. See ptc15.87.35040. Best to use Vpp instead of just size of a single phase when deciding on maxchan
- Another problem at ptc15.87.35040 is that it did actually detect the original teal chan 5 as a spike, but then when it went to look for phase2 on the new purple maxchan 6 and couldn't find it, it gave up completely instead of reverting back to the previous maxchan. Maybe I should save things that are definitely spike-like before looking for new maxchan, and revert to those if a PeakError is raised
- Neuron.update_wave() could be optimized by not looping over each ri separately, instead dealing with all ris at once
    - first loop took 9.417 sec, 2nd loop took 48.399 sec for a cluster with 92358 spikes - this is way too slow
    - consider aligning spikes in time in wavedata arrays, wrt their phase1 timepoint - can figure out which timepoint the spike starts at (go back phase1ti points from the reference point) and when it ends (count nt points from the start of the spike)
- better strategy for neuron chan selection: at least 1/2 the spikes have to contribute to a channel for it to be considered a neuron chan. Why are some obviously important chans being left out of some spikes? this is due to a full channel lockout due to a near simultaneous spike nearby
- add binary export of spike times and din for quick interoperation with neuropy
- store common surf record types (highpass, lowpass, and digitalsval) in structured ndarrays instead of long lists of Python objects
- when saving a .sort, and the only changes you've made are to the Sort itself (and maybe the nids in the spikes array), and no changes have been made to the spikes that have been detected and therefore to the wavedata, then just save the Sort object and the spikes array
    - this means you have to put the wavedata in the .sort file first?
    - might be best to split .sort file up into a .sort file (with spikes and Sort in it) and a .wave file (with wavedata in it)
        - make save .wave normal menu item, instead of the weird check box thing I have right now
        - this would make it easier to import a Sort and its clusters and apply them to a new set of spikes with their own wavedata
        - this would also make it more efficient to save multiple .sort files when testing various sorting methods (although not various detection methods) - each one wouldn't also have its huge associated wavedata duplicated unnecessarily
        - saving repeatedly while sorting would be much faster
- add file export item for exporting spikes and din and textheader, with a directory chooser
- tree is still really slow when expanding a neuron with 10s of 1000s of spikes - this is because it's not a true virtual tree like the virtual list is truly virtual - the tree populates all the children of all the currently expanded nodes, not just the currently visible children - that's cuz VirtualTree is implemented in Python - wx needs to do this at a lower level in C
    - best soln is probably to replace tree with 2 more virtual list ctrls: one listing all the neurons (clicking neurons displays their mean), and one listing the currently selected neuron's spikes (clicking selects that spike)
        - only downfall is you could no longer directly compare overplots of spikes of one neuron vs another, but this is already quite difficult to do with the multiselect tree
        - benefit would be doing away with the complexity of bugs in the tree, and most of all, speed!
        - another benefit is you could then sort neurons, and the spikes in each neuron, however you like by clicking appropriate columns in the appropriate virtual listctrl
        - another benefit is that listctrls can colour their items, to match neuron colour
- bug: mean neuron waveforms are being ignored on restore
- add a tooltip that displays nid, nspikes, etc when you hover over an ellipsoid
    - see https://svn.enthought.com/enthought/browser/Mayavi/trunk/examples/mayavi/data_interaction/select_points.py
        - also shows how to outline a selected point
    - easiest would be to use the picker and retrieve the scalar value for each ellipsoid. Need to set the value to something other than the default (1.0) when you generate it, make its value match nid (it's not being used for color or size or anything, so it won't affect the visualization) - problem is, dunno how to set it - really, you need to set the scalar value for all the 100s of points that make up the ellipsoid surface, because its any single one of those that you actually end up picking
- try if making get_spatial_mean take the peaks on each chan separately (but close in time to the peak on the max chan) improves clusterability - this would lump backprop spikes with their normal versions
    - yes, this seems to improve clusterability, in both ptc15 and ptc18
- apply a .sort file's neurons/clusters to the current sort
    - make this a File-Import neurons item?
- bug: clicking on a spike-less neuron in the nlist raises an error, which is fine, but then you can't plot any other neuron, which is bad - here's the traceback:












SPIKE MODELLING:
        - I think the real answer to most of my fitting problems is that I need to be able to place restraints on the parameters while the fit is running. Does this mean delving into leastsq fortran code? Rewriting leastsq in Cython? Is there another function I should use instead? Should do some searches, maybe mail the scipy list. - solution was to use openopt, which allows all kinds of constraints
        - disable grounded channels
        - lock each chan out separately wrt to its phase2ti, since all chans can now be potentially delayed wrt source - this will require calculating each channel's phase2ti separately...
        - take weighted spatial mean of chanis at phase1ti to better estimate initial (x0, y0)
        - if giving phase1 and phase2 different AP velocity delays, should probably constrain them to be of the same sign - scratch that: see ptc15.87.26940 where the 1st phase increasingly leads the spike time as a f'n of distance, and the 2nd phase increasingly lags the spike time as a f'n of distance. You need to allow v1inv and v2inv to be of opposite sign to successfully model this
        - constrain that sx and sy need to be within some factor of each other, ie constrain their ratio
        - improve estimation of window limits relative to threshold crossing
        - add AP velocity param(s) - make that inverse velocity to avoid singularity
            - seems to be causing some x localization problems...
        - improve spatiotemporal lockout accuracy
            - for every fit spike, check if the max fit val is a peak (ie has values on either side that are lower than it). Or, just check if the means of both phases fall within the window limits. If not, chances are good that this thresh event was triggered by a late channel that's only now crossed thresh, just outside of the spatial lockout of the actual spike, which had already been modelled. Therefore, the currently fit spike should be thrown out
                - maybe a better way to deal with this late channel situation is to do the AP velocity modelling - that way each modelled chan can be locked out to a different timepoint according to the modelled propogation delay
        - test if constraints are actually making any difference, or if ralg is just amazing
        - search for maxchani within chanis, and recenter on it, generating a new more appropriate set of chanis. Then take weighted spatial mean of these chanis at phase1t to get best estimate of source. Feed this best estimate as (x0, y0) into model - ie get away from maxchan centric thinking
        - nah: regenerate chanis list based on this spatial mean based best estimate
            - don't think this will make any significant difference to model results
        - NAH: try 1/r model again, with initial guess located at weighted spatial mean instead of at maxchan, to avoid singularity at maxchan
            - or, replace param r with rinv, or "nearness" param, to get rid of singularity, like I did for AP velocity v and vinv ("slowness")
        - run profiler - R algorithm is just plain slow - solution is to try another one
        - should probably initialize model (x0, y0) to weighted spatial mean of chans, instead of just the maxchan coords - this would also help to reduce singularity/differentiability problems with 1/r model

